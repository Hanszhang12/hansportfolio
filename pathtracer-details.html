<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">

  <title>Portfolio Details - Pathtracer</title>
  <meta content="" name="description">
  <meta content="" name="keywords">

  <!-- Favicons -->
  <link href="assets/img/favicon.png" rel="icon">
  <link href="assets/img/apple-touch-icon.png" rel="apple-touch-icon">

  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i|Raleway:300,300i,400,400i,500,500i,600,600i,700,700i|Poppins:300,300i,400,400i,500,500i,600,600i,700,700i" rel="stylesheet">

  <!-- Vendor CSS Files -->
  <link href="assets/vendor/aos/aos.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
  <link href="assets/vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
  <link href="assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
  <link href="assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">

  <!-- Template Main CSS File -->
  <link href="assets/css/style.css" rel="stylesheet">


</head>

<body>

  <main id="main">

    <!-- ======= Portfolio Details Section ======= -->
    <section id="portfolio-details" class="portfolio-details">
      <div class="container">

        <div class="row gy-4">

          <div class="col-lg-6 offset-lg-3">
            <div class="portfolio-details-slider swiper">
              <div class="swiper-wrapper align-items-center">

                <div class="swiper-slide">
                  <img src="assets/img/pathtracer/pathtracer1.png" alt="">
                </div>

                <div class="swiper-slide">
                  <img src="assets/img/pathtracer/pathtracer2.png" alt="">
                </div>

                <div class="swiper-slide">
                  <img src="assets/img/pathtracer/pathtracer3.png" alt="">
                </div>

                <div class="swiper-slide">
                  <img src="assets/img/pathtracer/pathtracer4.png" alt="">
                </div>

                <div class="swiper-slide">
                  <img src="assets/img/pathtracer/pathtracer5.png" alt="">
                </div>

                <div class="swiper-slide">
                  <img src="assets/img/pathtracer/pathtracer6.png" alt="">
                </div>



              </div>
              <div class="swiper-pagination"></div>
            </div>
          </div>

          <div class="col-lg-12">
            <div class="portfolio-info">
              <h3>Pathtracer</h3>
              <ul>
                <li><strong>Category</strong>: Graphics</li>
                <li><strong>Project URL</strong>: <a href="https://github.com/Hanszhang12/Pathtracer">https://github.com/Hanszhang12/Pathtracer</a></li>
              </ul>
            </div>
            <div class="portfolio-description">
              <h2>Project Details</h2>
              <p>
              In this project, we worked on various aspects of the ray-tracing pipeline and implemented efficient shading of surfaces and objects. We began this endeavor by working according to the first principle of ray-tracing, generating rays and their intersections with objects. We used the bounding volume hierarchy data structure (BVH) in order to necessarily speed up long tests. We then implemented calculations for direct illumination (zero bounce and one bounce) based on physical rules of light and our ray implementations. We complemented these calculations with their counterpart: implementations for indirect illumination which procured seamless images using global illumination principles. Lastly, we performed an optimization called “adaptive sampling” in order to short circuit sampling once pixels have converged to an appropriate level.
              </p>

              <p><strong>Ray Generation and Intersection </strong></p>
              <p>Our ray generation algorithm consists of two steps. First, we converted the input world image coordinates to camera coordinates by using the transformation (tan(hFov/2), tan(vFov/2), -1). Secondly, we converted the transformed coordinates into world space and then normalized them. In order to generate pixel samples, we had to change the underlying coordinate system from the world space to the image space by dividing the given (x, y) coordinates by the width and height of the image space respectively. Keeping in mind that the origin of the coordinate space is the bottom left corner, we iterated over the number of samples and used the gridSampler object to attain new sample points to pass into the generate ray function. We then used these points to update our sample and sampleCount buffers.
              In order to test for triangle intersection, we manipulated the 3D triangles’ normal and area properties to compute barycentric coordinates to then test if a point evaluated at time t is inside a given triangle. Using the cross product of the differences between the triangle’s vertices, we computed the normal of the triangle which we then used to compute the intersection time t from the ray equation (p^’ - o) (dot)N / (d (dot) N). We had to verify that this intersection time t lay in the appropriate interval (min_t, max_t). We finally derived the barycentric coordinates using proportions of the triangles’ areas which are equal to the squared norm of the corresponding normal vectors. Finally, we verified that the resulting coefficients were between 0 and 1 and then updated the arguments for isect with the discovered information. Sphere intersection tests are extremely similar with the caveat that intersection tests for a given point may be computed by using the ellipsoid equation (p-c)^2 <= r^2. And so the intersection values t_min and t_max simply become the roots of the final quadratic equation.
              </p>
              <p><strong>Bounding Volume Hierarchy</strong></p>
              <p>The first thing I needed to do was get the number of primitives and the size of the BBox. I did this by using a for loop that expanded the current BBox and also added to the primitive count. We also had to set the start and end pointers to be the ones passed into the function. Afterwards, I check to see if the primitive count is less than the max leaf size. If it is, we set the left and right nodes of the main node to be NULL. If it wasn’t, we would need to split up start and end into two branches where we would recursively call construct_bvh. Originally, I wanted to split it up based on the middle centroid of the longest axis. However, I was running into too many segfault errors so I decided to just split it by the median primitive.

              The cow took around 40 seconds to run without BVH acceleration and less than a second with BVH acceleration. The more complex scenes such as the maxplanck would take a very long time to render but can render in a matter of a few seconds using BVH. BVH acceleration is much faster than the original construction because it runs in log(N) time as opposed to (N) time. It recursively splits up the tree in two at each level, making it run in log time.
              </p>
              <p><strong>Direct Illumination</strong></p>
              <p>For the implementation of the f function, we simply returned reflectance/pi due to uniformity. The returned zero bound illumination was just the BSDF output for an intersection. For direct lighting with uniform hemisphere sampling, the uniform hemisphere concept implies a pdf of 1/2pi, which is the pdf we then used. We then iterated over num_samples and for each iteration we sampled a ray direction and converted it to the world basis so that we would have the ray in the correct spatial point. Using the radiance equation from lecture, we then summed up the radiances per iteration and divided this summed value by num_samples for the final integral value. For direct lighting by importance sampling lights, we looped over all ScenceLight objects and generated rays in the same way as before for ns_area_light samples per SceneLight object. The difference between the approach here and that of uniform hemisphere sampling is that we project the angle in an opposite direction, have an arbitrary pdf rather than a uniform one, and keep track of the distance to the light at a given point in time from which we then set the parameter max_t appropriately. We then check for an intersection, use the reflectance equation to evaluate radiance, and then normalize the light value by the number of samples for that iteration. Lastly, we return the summed value of all the normalized radiances.
              The benefit of importance sampling is that it uses useful information which allows it to converge with less noise and less ray usage. The uniform approach casts rays in random directions, which leads to a larger amount of noise and “misfires” or uninformative casts.
              </p>
              <p><strong>Mirror and Glass Materials</strong></p>
              <p>For the image corresponding to m = 0, we just see the light at the top. This is due to the fact that since there’s no bounce, nothing in the image is illuminated. For m = 1, we can make out the spheres but they remain black because the max ray depth is only one. This means that the limited max ray depth prevents the light from bouncing off objects. For m = 2, we can see the mirror sphere but the glass sphere still remains unclear because the bounce number is still too low to pass through the glass sphere. For m = 3, we can now see the glass sphere clearly. However, the reflection of the glass sphere in the mirror sphere is still a bit unclear. We can also see the light passing through the glass sphere. For m = 4, we can see the glass sphere clearly in the mirror sphere. We can also see some of the light refract on the right wall. Max ray depth 5 and 100 are pretty similar but a bit brighter than the previous images.
              </p>
              <p><strong>Microfacet Material</strong></p>
              <p>A decreased alpha value pertains to a more mirror-like and smooth appearance, while larger alpha values correspond to a brighter yet less mirror-like and more opaque image.
              The bunny rendered with importance sampling clearly has smoother edges and contours that provide the bunny with a more realistic 3D shape and color. Furthermore, the lighting of the importance-sampling bunny is more realistic given the scene at hand and the light above the bunny. The cosine sampling bunny has a strange dark aura surrounding it and has a less smooth surface in contrast to the former.
              In order to render a silver bunny, I used the eta parameters: (0.0592, 0.0599, 0.0474) and the k parameters: (4.1283, 3.5892, 2.8132) which I figured out by googling how to represent silver surfaces. As shown in the image, the bunny appears less mirror-like and has a more silvery sheen to it.
              </p>
            </div>
          </div>

        </div>
      </div>
    </section><!-- End Portfolio Details Section -->
  </main><!-- End #main -->

  <div id="preloader"></div>
  <a href="#" class="back-to-top d-flex align-items-center justify-content-center"><i class="bi bi-arrow-up-short"></i></a>

  <!-- Vendor JS Files -->
  <script src="assets/vendor/purecounter/purecounter_vanilla.js"></script>
  <script src="assets/vendor/aos/aos.js"></script>
  <script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="assets/vendor/glightbox/js/glightbox.min.js"></script>
  <script src="assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
  <script src="assets/vendor/swiper/swiper-bundle.min.js"></script>
  <script src="assets/vendor/waypoints/noframework.waypoints.js"></script>
  <script src="assets/vendor/php-email-form/validate.js"></script>

  <!-- Template Main JS File -->
  <script src="assets/js/main.js"></script>

</body>

</html>
